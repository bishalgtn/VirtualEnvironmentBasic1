import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re

def check_security_headers(headers):
    """
    Check for the presence of HTTP security headers.
    :param headers: Dictionary of HTTP response headers.
    :return: List of missing headers.
    """
    required_headers = ["X-Content-Type-Options", "Strict-Transport-Security"]
    missing_headers = [header for header in required_headers if header not in headers]
    return missing_headers

def check_forms_for_security(soup):
    """
    Check forms on a page for missing or insecure attributes.
    :param soup: BeautifulSoup object of the page's HTML.
    :return: List of insecure forms.
    """
    issues = []
    forms = soup.find_all("form")
    for form in forms:
        method = form.get("method", "GET").upper()
        action = form.get("action", None)
        if method != "POST":
            issues.append("FORM WITH UNSAFE METHOD ATTRIBUTE (NOT POST)")
        if not action:
            issues.append("FORM WITH MISSING ACTION ATTRIBUTE")
    return issues

def check_for_outdated_versions(headers, content):
    """
    Check for outdated software versions in HTTP headers or page content.
    :param headers: Dictionary of HTTP response headers.
    :param content: HTML content of the page.
    :return: List of detected outdated software versions.
    """
    outdated_versions = []
    # Example of checking for outdated Apache versions in headers
    server = headers.get("Server", "")
    if "Apache/2.4.6" in server:
        outdated_versions.append("OUTDATED SOFTWARE VERSION DETECTED: APACHE 2.4.6")

    # Example of checking page content for known outdated versions
    if re.search(r"Apache/2\.4\.6", content, re.IGNORECASE):
        outdated_versions.append("OUTDATED SOFTWARE VERSION DETECTED IN PAGE CONTENT: APACHE 2.4.6")

    return outdated_versions

def crawl_and_scan(url, visited_urls=set()):
    """
    Crawl the given URL and scan for vulnerabilities.
    :param url: URL to start crawling from.
    :param visited_urls: Set of already visited URLs to prevent loops.
    :return: List of vulnerabilities found.
    """
    vulnerabilities = []
    try:
        # Fetch the page
        response = requests.get(url)
        soup = BeautifulSoup(response.content, "html.parser")
        visited_urls.add(url)

        # Check for vulnerabilities
        vulnerabilities.extend(check_security_headers(response.headers))
        vulnerabilities.extend(check_forms_for_security(soup))
        vulnerabilities.extend(check_for_outdated_versions(response.headers, response.text))

        # Find all links on the page and crawl them recursively
        links = [a.get("href") for a in soup.find_all("a", href=True)]
        for link in links:
            absolute_url = urljoin(url, link)
            # Ensure the link belongs to the same domain and hasn't been visited
            if urlparse(absolute_url).netloc == urlparse(url).netloc and absolute_url not in visited_urls:
                vulnerabilities.extend(crawl_and_scan(absolute_url, visited_urls))

    except requests.exceptions.RequestException as e:
        print(f"Error accessing {url}: {e}")

    return vulnerabilities

if __name__ == "__main__":
    start_url = input("Enter the URL to scan: ").strip()
    print(f"Starting scan for {start_url}...\n")
    vulnerabilities = crawl_and_scan(start_url)

    if vulnerabilities:
        print("VULNERABILITIES FOUND:")
        for vulnerability in vulnerabilities:
            print(f"- {vulnerability}")
    else:
        print("No vulnerabilities found.")
